\relax 
\citation{VogelsteinPriebe10}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{sec:introduction}{{1}{1}}
\newlabel{h1}{{1}{1}}
\newlabel{h2}{{2}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}A possible description of brains with certain desirable properties}{2}}
\newlabel{sec:B}{{2}{2}}
\citation{Stone77}
\citation{VogelsteinPriebe10}
\citation{?}
\citation{MaaBartoszynski96}
\citation{Breiman01}
\citation{?}
\@writefile{toc}{\contentsline {section}{\numberline {3}Possible approaches to choosing mappings with desirable properties}{3}}
\newlabel{sec:map}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Model-free algorithms}{3}}
\newlabel{sub:model_free_algorithms}{{3.1}{3}}
\citation{AllmanRhodes10}
\citation{DGL96}
\citation{?}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Model-based algorithm}{4}}
\newlabel{sub:model_based_algorithm}{{3.2}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Edge independent models}{4}}
\newlabel{ssub:edge_indep}{{3.2.1}{4}}
\citation{?}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Edge conditionally independent model}{5}}
\newlabel{ssub:edge_cond}{{3.2.2}{5}}
\citation{Durbin87}
\citation{Durbin87}
\citation{deBonoMaricq05}
\newlabel{eq:link}{{8}{6}}
\@writefile{toc}{\contentsline {paragraph}{Unobserved edges and features}{6}}
\@writefile{toc}{\contentsline {paragraph}{Kidney and egg like special case}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{6}}
\newlabel{sec:results}{{4}{6}}
\citation{VarshneyChklovskii09}
\citation{ChalasaniBargmann07}
\citation{VaziriShank08}
\citation{HelmstaedterDenk08}
\citation{Mishchenko09}
\citation{LuLichtman09}
\citation{deBonoMaricq05}
\citation{BuckinghamSattelle08}
\bibdata{/Users/joshyv/Research/misc/biblist}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces C.\nobreakspace  {}elegans graph classification simulation results. $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle L$}\mathaccent "0362{L}^{1000}_{F}(g_n)$ is plotted as a function of class-conditional training sample size $n_j$, suggesting that for $\varepsilon =0.1$ we can determine that $\@mathcal {M}\begingroup \setbox \z@ \hbox {\thinmuskip 0mu \medmuskip \m@ne mu\thickmuskip \@ne mu \setbox \tw@ \hbox {${\sim }\mathsurround \z@ $}\kern -\wd \tw@ ${}{\sim }{}\mathsurround \z@ $}\edef [{\endgroup \let \binrel@@ \relax }[\binrel@@ {\mathop {\kern \z@ {\sim }}\limits ^{\varepsilon }}_F \@mathcal {B}$ holds with $99\%$ confidence with just a few hundred training samples generated from $F_{BM}$. Each dot depicts an estimate for $L_{F}(g_n)$; standard errors are $(L_{F}(g_n)(1-L_{F}(g_n))/1000)^{1/2}$. E.g., $n_j = 180$ ; $k_n = 53$ ; $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle L$}\mathaccent "0362{L}^{1000}_{F}(g_n) = 0.057$; standard error less than 0.01. We reject $H_0: L_{F}(g^*) \geq 0.10$ at $\alpha =0.01$. $L_{F}(g^*) \approx 0$ for this simulation.}}{7}}
\newlabel{fig1}{{1}{7}}
\bibcite{VogelsteinPriebe10}{1}
\bibcite{Stone77}{2}
\bibcite{MaaBartoszynski96}{3}
\bibcite{Breiman01}{4}
\bibcite{AllmanRhodes10}{5}
\bibcite{DGL96}{6}
\bibcite{Durbin87}{7}
\bibcite{deBonoMaricq05}{8}
\bibcite{VarshneyChklovskii09}{9}
\bibcite{ChalasaniBargmann07}{10}
\bibcite{VaziriShank08}{11}
\bibcite{HelmstaedterDenk08}{12}
\bibcite{Mishchenko09}{13}
\bibcite{LuLichtman09}{14}
\bibcite{BuckinghamSattelle08}{15}
\bibstyle{ieeetr}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{8}}
\newlabel{sec:discussion}{{5}{8}}
\@writefile{toc}{\contentsline {paragraph}{Acknowledgments}{8}}
\@writefile{toc}{\contentsline {section}{References}{8}}
